\section{Introduktion til multipel regression}

Fra simpel \href{http://www.webmatematik.dk/lektioner/matematik-b/regression}{lineær
regressions} analyse ved vi, hvordan man med
\href{http://www.webmatematik.dk/lektioner/matematik-b/regression}{mindste kvadraters metoden}
bestemmer den lineære funktion, som passer bedst til en række observationer i 2D planen.

Vi har altså her en række observationer \((y_i, x_i)\) for \(i=1,\ldots,n\) og ønsker at bestemme konstanterne \(a\) og \(b\) på en sådan måde, at den lineære funktion
\begin{displaymath}
  y = b + a x
\end{displaymath}
ligger så tæt på alle observationer \((y_i, x_i)\) som muligt.

Verden er dog sjældent så simpelt indrettet, at man kan beskrive en afhængig variabel \(y\)
med kun en enkelt forklarende variabel \(x\).

Multipel regression er en udvidelse af simpel regression, hvor vi i stedet for en enkelt forklarende
variabel har to eller flere forklarende variable. Forklarende varibale kaldes til tider også for kovarianter.

Lad os se på et (simplificeret) eksempel på multipel regressionsanalyse. Eksemplet er hentet fra
opgaven ``Prisdannelse på ejerlejligheder i København'' af Vibeke Stål og Anne Melvej Stennevad.
Forfatterne undersøger i projektet om der er en lineær sammenhæng mellem prisen på ejerlejligheder i København og en række faktorer såsom fx renteniveau og byggeomkostninger.

\begin{displaymath}
  pris = b_0 + b_1 * byggeomkostninger + b_2 * renteniveau
\end{displaymath}


hvad bruger vi regression til - forudsige



For en række observationer \(y_i,x_{1,i},x_{2,i},\ldots,x_{p,i}\) hvor \(i=1,\ldots,n\) ønsker vi at bestemme konstanter \(b_0,b_1,b_2,\ldots,b_p\), så funktionen
\begin{displaymath}
  y = b_0 + b_1 x_1 + b_2 x_2 + \cdots b_p x_p
\end{displaymath}
ligger så tæt på alle punkterne \(y_i,x_{1,i},x_{2,i},\ldots,x_{p,i}\) som muligt.

Hvis koefficienterne \(b_0,b_1,b_2,\ldots,b_p\) er kendt, kan vi bruge ovennævnte formel
make predictions using the formula
\begin{displaymath}
\hat{y} = \hat{b_0} + \hat{b_1}x_1 + \hat{b_2}x_2 + \cdots + \hat{b_p}x_p
\end{displaymath}

Koefficienter bliver normal fundet (estimeret) ved brug af mindste kvadrater metoden, dvs man vælger \(b_0, b_1, \ldots , b_p\)  således at udtrykket
\begin{displaymath}
  RSS = \sum^n_{i=1} (y_i - \hat{y_i})^2 = \sum^n_{i=1} (y_i - \hat{b_0} - \hat{b_1}x_{i1} - \hat{b_2} x_{i2} - \dots - \hat{b_p} x_{ip})^2
\end{displaymath}
minimeres. RSS er engelsk for Residuals Sum of Squares. Dette er samme fremgangsmetode som kendes fra simpel lineær regression.

$b_0$ intercept - skæring
andre koefficienter - hældning

ingen formel da den er ret kompliceret

praktisk eksempel gennemgang i Excel

mere formel definition

sammenlign med lineær regression
hvor man fitter linie
her fitter man plan hvis vi er i 3D
Visualization of the Multiple Regression Model

It is more difficult to visualize multiple regression than simple regression.  In the case where you have two independent (explanatory) variables the "line" of best fit becomes a two dimensional surface (i.e. a plane).



pas på med multicollinearity.

\subsubsection{Historie}


\subsection{Korrelationskoefficient}
altid muligt at bestemme koeffienter men giver det mening,
er der en sammenhæng mellem x og y.

\subsubsection{Formel for korrelationskoefficient for 2 uafhængige variable}

\subsubsection{Fortolkning af korrelationskoefficient}

\begin{displaymath}
  R = \frac{\sqrt{r^2_{yx_1} + r^2_{yx_2} - 2r_{yx_1} r_{yx_2} r_{{x_1}x_2}}}{\sqrt{1 - r^2_{{x_1}x_2}}}
\end{displaymath}

\subsection{Determinationskoefficient}

kaldes også forklaringsgraden

\subsubsection{Formel}


\subsection{Residualplot}


\subsection{Konfidensinterval for parametre}


\subsection{CAS fx R}

\subsection{brug din fornuft}
counterexample

kig på graf

\subsection{model selection}

\subsection{adjusted \(R^2\)}
