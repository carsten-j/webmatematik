% !TEX root = webmatematik.tex

\section{Multipel regression}

\subsection{Introduktion}
foo
Fra simpel \href{http://www.webmatematik.dk/lektioner/matematik-b/regression}{lineær
regressions} analyse ved vi, hvordan man med
\href{http://www.webmatematik.dk/lektioner/matematik-b/regression}{mindste kvadraters metoden}
bestemmer den lineære funktion, som bedst passer til en række observationer i 2D planen.

Vi har altså her observationer \((y_i, x_i)\) for \(i=1,\ldots,n\) og ønsker at bestemme konstanterne \(a\) og \(b\) på en sådan måde, at den lineære funktion
\begin{displaymath}
  y = b + a x
\end{displaymath}
ligger så tæt på alle observationer \((y_i, x_i)\) som muligt.

Verden er dog sjældent så simpelt indrettet, at man kan beskrive en \textit{afhængig} variabel \(y\)
med kun en enkelt \textit{forklarende} variabel \(x\).

Multipel regression er en udvidelse af simpel regression, hvor vi i stedet for en enkelt forklarende
variabel har to eller flere forklarende variable. For\-kla\-ren\-de variable kaldes til tider også for \textit{kovarianter}.

\subsection{Model}
Vi har \(n\) observationer \(y_i,x_{1,i},x_{2,i},\ldots,x_{p,i}\) hvor \(i=1,\ldots,n\) og ønsker at bestemme konstanter \(b_0,b_1,b_2,\ldots,b_p\), så funktionen
\begin{displaymath}
  y = b_0 + b_1 x_1 + b_2 x_2 + \cdots b_p x_p
\end{displaymath}
ligger så tæt på alle punkterne \(y_i,x_{1,i},x_{2,i},\ldots,x_{p,i}\) som muligt.

Bemærk, at der nu er et dobbelt indeks på \(x\)'erne. Det er nødvendigt, da vi nu istedet for en enkelt forklarende variabel, har \(p\) forklarende variable. Så når vi skriver

\begin{displaymath}
  x_{ji}, \quad j=1,\ldots,p \textrm{ og } i=1,\ldots,n
\end{displaymath}
er der tale om den \(j\)'te forklarende variable og den \(i\)'te observation.

Koefficienter bliver normal fundet (estimeret) ved brug af mindste kvadrater metoden, dvs man vælger \(b_0, b_1, \ldots , b_p\)  således at udtrykket
\begin{displaymath}
  RSS =\sum^n_{i=1} \big (y_i - \widehat{b_0} - \widehat{b_1} x_{i1} - \widehat{b_2} x_{i2} - \dots - \widehat{b_p} x_{ip} \big)^2
\end{displaymath}
minimeres. RSS er engelsk for Residuals Sum of Squares. Dette er samme fremgangsmetode som kendes fra simpel lineær regression.

Formlen for koefficienterne \(b_0,b_1,b_2,\ldots,b_p\) er noget mere kompliceret i det generelle tilfælde, så den springer vi over her. Men nedenfor viser vi et eksempel på, hvordan man kan bestemme koefficienter ved brug af Microsoft Excel.

Sommetider omtales \(b_0\) som skæringen og de øvrige \(b\)'er som hældningen.

I tilfældet med simpel regression bestemmer vi en ret linie som passer bedst til observationerne. Det er umuligt at visualisere multipel regression i det generelle tilfælde. For det særlige tilfælde, hvor vi har to forklarende variable \(x_1\) og \(x_2\) og dermed modellen

\begin{displaymath}
  y = b_0 + b_1 x_1 + b_2 x_2
\end{displaymath}

så kan vi stadig visualisere løsningen. Vi kan tænke på observationerne \((x_{1i}, x_{2i}, y_i), i=1,\ldots,n\) som punkter i rummet

INDSÆT TEGNING

Løsningen er nu ikke længere en ret linie, men derimod den plan som ligger tættest på alle punkterne.




Hvis koefficienterne \(b_0,b_1,b_2,\ldots,b_p\) er kendt, kan vi bruge ovennævnte formel
make predictions using the formula
\begin{displaymath}
\hat{y} = \hat{b_0} + \hat{b_1}x_1 + \hat{b_2}x_2 + \cdots + \hat{b_p}x_p
\end{displaymath}

Lad os se på et (simplificeret) eksempel på multipel regressionsanalyse. Eksemplet er hentet fra
opgaven ``Prisdannelse på ejerlejligheder i København'' af Vibeke Stål og Anne Melvej Stennevad.
Forfatterne undersøger, om der er en lineær sammenhæng mellem prisen på ejerlejligheder i København og en række faktorer såsom fx renteniveau og byggeomkostninger.

\begin{displaymath}
  pris = b_0 + b_1 * byggeomkostninger + b_2 * renteniveau
\end{displaymath}


hvad bruger vi regression til - forudsige







praktisk eksempel gennemgang i Excel

mere formel definition

sammenlign med lineær regression
hvor man fitter linie
her fitter man plan hvis vi er i 3D
Visualization of the Multiple Regression Model

It is more difficult to visualize multiple regression than simple regression.  In the case where you have two independent (explanatory) variables the "line" of best fit becomes a two dimensional surface (i.e. a plane).



pas på med multicollinearity.

\subsubsection{Historie}


\subsection{Korrelationskoefficient}
altid muligt at bestemme koeffienter men giver det mening,
er der en sammenhæng mellem x og y.

\subsubsection{Formel for korrelationskoefficient for 2 uafhængige variable}

\subsubsection{Fortolkning af korrelationskoefficient}

\begin{displaymath}
  R = \frac{\sqrt{r^2_{yx_1} + r^2_{yx_2} - 2r_{yx_1} r_{yx_2} r_{{x_1}x_2}}}{\sqrt{1 - r^2_{{x_1}x_2}}}
\end{displaymath}

\subsection{Determinationskoefficient}

kaldes også forklaringsgraden

\subsubsection{Formel}


\subsection{Residualplot}


\subsection{Konfidensinterval for parametre}


\subsection{CAS fx R}

\subsection{brug din fornuft}
counterexample

kig på graf

\subsection{model selection}

\subsection{adjusted \(R^2\)}
